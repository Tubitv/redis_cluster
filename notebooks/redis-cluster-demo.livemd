<!-- livebook:{"file_entries":[{"name":"hash-slots.png","type":"attachment"},{"name":"tree-diagram.png","type":"attachment"}],"persist_outputs":true} -->

# Redis Cluster Demo

```elixir
Mix.install([
  {:redis_cluster, path: Path.expand("../", __DIR__)},
  {:kino_vega_lite, "~> 0.1.13"}
])
```

## Summary

Redis is an in-memory key-value store. In it's most basic form, it has a single server that contains all the key-value pairs. It can optionally have up to 5 read replicas (at least for AWS ElastiCache). Expanding beyond this requires cluster mode, which introduces some new complexity.

## Setup

To demonstrate, let's first start up a cluster. 
It will have 12 nodes running from ports 7000-7011. 
There will be 4 masters with 2 replicas each. 
You must have `redis-cli` installed first (`brew install redis`).

```elixir
exec = Path.expand("../scripts/redis_cluster.exs", __DIR__)

{output, 0} = System.cmd(exec, ~w[start --port 7000 --replicas-per-master 2])

IO.puts(output)
```

<!-- livebook:{"output":true} -->

```
Starting Redis on port 7000
Starting Redis on port 7001
Starting Redis on port 7002
Starting Redis on port 7003
Starting Redis on port 7004
Starting Redis on port 7005
Starting Redis on port 7006
Starting Redis on port 7007
Starting Redis on port 7008
Starting Redis on port 7009
Starting Redis on port 7010
Starting Redis on port 7011
Waiting for Redis instances to start...
Creating Redis Cluster with nodes: ["127.0.0.1:7000", "127.0.0.1:7001", "127.0.0.1:7002", "127.0.0.1:7003", "127.0.0.1:7004", "127.0.0.1:7005", "127.0.0.1:7006", "127.0.0.1:7007", "127.0.0.1:7008", "127.0.0.1:7009", "127.0.0.1:7010", "127.0.0.1:7011"]

```

<!-- livebook:{"output":true} -->

```
:ok
```

Next, we connect to the cluster.

```elixir
config = %RedisCluster.Configuration{
  host: "localhost",
  port: 7000,
  name: Test.Redis,
  registry: Test.Redis.Registry__,
  pool: Test.Redis.Pool__,
  cluster: Test.Redis.Cluster__,
  shard_discovery: Test.Redis.ShardDiscovery__,
  pool_size: 3
}

pid = case RedisCluster.Cluster.start_link(config) do
  {:ok, pid} -> pid
  {:error, {:already_started, pid}} -> pid
end
```

<!-- livebook:{"output":true} -->

```
#PID<0.374.0>
```

This starts a supervisor that watches connection pools, among other things. 
Re-run the following cell if you don't see all the connections.
It takes a moment to discover them all.

```elixir
Kino.Process.render_sup_tree(pid, direction: :left_right)
```

<!-- livebook:{"output":true} -->

```mermaid
graph LR;
2(Test.Redis.Pool__):::supervisor ---> 23(#PID<0.436.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 35(#PID<0.448.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 15(#PID<0.426.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 9(#PID<0.434.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 37(#PID<0.442.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 6(#PID<0.452.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 36(#PID<0.391.0>):::worker
17(#PID<0.406.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 31(#PID<0.418.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 7(#PID<0.410.0>):::worker
20(#PID<0.424.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 24(#PID<0.428.0>):::worker
22(#PID<0.395.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 3(#PID<0.408.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 18(#PID<0.393.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 22(#PID<0.395.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 5(#PID<0.446.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 28(#PID<0.420.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 32(#PID<0.422.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 38(#PID<0.438.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 26(#PID<0.450.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 29(#PID<0.387.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 35(#PID<0.448.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 26(#PID<0.450.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 20(#PID<0.424.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 17(#PID<0.406.0>):::worker
13(#PID<0.440.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 14(#PID<0.404.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 36(#PID<0.391.0>):::worker
21(#PID<0.381.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
4(#PID<0.430.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 28(#PID<0.420.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 27(#PID<0.402.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 24(#PID<0.428.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 13(#PID<0.440.0>):::worker
23(#PID<0.436.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
11(#PID<0.383.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 11(#PID<0.383.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 27(#PID<0.402.0>):::worker
14(#PID<0.404.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
15(#PID<0.426.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 10(#PID<0.385.0>):::worker
12(#PID<0.416.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 38(#PID<0.438.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 31(#PID<0.418.0>):::worker
19(#PID<0.398.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
0(Test.Redis.Cluster__):::root ---> 2(Test.Redis.Pool__):::supervisor
39(Test.Redis.Registry__):::supervisor ---> 40(Test.Redis.Registry__.PIDPartition0):::worker
5(#PID<0.446.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
9(#PID<0.434.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 33(#PID<0.414.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 25(#PID<0.389.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 29(#PID<0.387.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 25(#PID<0.389.0>):::worker
0(Test.Redis.Cluster__):::root ---> 1(Test.Redis.ShardDiscovery__):::worker
0(Test.Redis.Cluster__):::root ---> 39(Test.Redis.Registry__):::supervisor
2(Test.Redis.Pool__):::supervisor ---> 12(#PID<0.416.0>):::worker
16(#PID<0.444.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 34(#PID<0.412.0>):::worker
7(#PID<0.410.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 4(#PID<0.430.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 34(#PID<0.412.0>):::worker
6(#PID<0.452.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 30(#PID<0.400.0>):::worker
3(#PID<0.408.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 19(#PID<0.398.0>):::worker
40(Test.Redis.Registry__.PIDPartition0):::worker -..- 30(#PID<0.400.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 21(#PID<0.381.0>):::worker
8(#PID<0.432.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
10(#PID<0.385.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 37(#PID<0.442.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 33(#PID<0.414.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 16(#PID<0.444.0>):::worker
2(Test.Redis.Pool__):::supervisor ---> 8(#PID<0.432.0>):::worker
18(#PID<0.393.0>):::worker -..- 40(Test.Redis.Registry__.PIDPartition0):::worker
2(Test.Redis.Pool__):::supervisor ---> 32(#PID<0.422.0>):::worker
classDef root fill:#c4b5fd, stroke:#374151, stroke-width:4px, line-height:1.5em;
classDef supervisor fill:#c4b5fd, stroke:#374151, stroke-width:1px, line-height:1.5em;
classDef worker fill:#66c2a5, stroke:#374151, stroke-width:1px, line-height:1.5em;
classDef notstarted color:#777, fill:#d9d9d9, stroke:#777, stroke-width:1px, line-height:1.5em;
classDef ets fill:#a5f3fc, stroke:#374151, stroke-width:1px;


```

Then, we confirm the Redis cluster topology.

```elixir
config
|> RedisCluster.HashSlots.all_slots()
|> Enum.sort()
```

<!-- livebook:{"output":true} -->

```
[
  {RedisCluster.HashSlots, 0, 5460, :master, "127.0.0.1", 7011},
  {RedisCluster.HashSlots, 0, 5460, :replica, "127.0.0.1", 7000},
  {RedisCluster.HashSlots, 0, 5460, :replica, "127.0.0.1", 7003},
  {RedisCluster.HashSlots, 0, 5460, :replica, "127.0.0.1", 7006},
  {RedisCluster.HashSlots, 5461, 10922, :master, "127.0.0.1", 7010},
  {RedisCluster.HashSlots, 5461, 10922, :replica, "127.0.0.1", 7001},
  {RedisCluster.HashSlots, 5461, 10922, :replica, "127.0.0.1", 7007},
  {RedisCluster.HashSlots, 5461, 10922, :replica, "127.0.0.1", 7009},
  {RedisCluster.HashSlots, 10923, 16383, :master, "127.0.0.1", 7005},
  {RedisCluster.HashSlots, 10923, 16383, :replica, "127.0.0.1", 7002},
  {RedisCluster.HashSlots, 10923, 16383, :replica, "127.0.0.1", 7004},
  {RedisCluster.HashSlots, 10923, 16383, :replica, "127.0.0.1", 7008}
]
```

![](files/tree-diagram.png)

## Hash Slots

In clustered mode, the key is run through a CRC-16 X-Modem function modulo 16,384. This gives a range of 0-16,383. This keyspace is split up among all the master nodes.

Let's query the cluster and see what this looks like.

```elixir
config
|> RedisCluster.HashSlots.all_slots()
|> Enum.filter(fn {_mod, _lo, _hi, role, _host, _port} -> role == :master end)
|> Enum.sort()
```

<!-- livebook:{"output":true} -->

```
[
  {RedisCluster.HashSlots, 0, 5460, :master, "127.0.0.1", 7011},
  {RedisCluster.HashSlots, 5461, 10922, :master, "127.0.0.1", 7010},
  {RedisCluster.HashSlots, 10923, 16383, :master, "127.0.0.1", 7005}
]
```

![](files/hash-slots.png)

<!-- livebook:{"break_markdown":true} -->

With a fresh cluster, we should have four ranges:

* 0-4095
* 4096-8191
* 8192-12,287
* 12,288-16,383

If a cluster adds and removes shards, these ranges can become fragmented. For simplicity in the examples, we'll stick with these nice ranges.

Now, let's take some arbitrary keys and see what they hash to.

```elixir
for key <- ~w[1 2 3 4 5] do
  slot = RedisCluster.Key.hash_slot(key)
  IO.puts("#{key} => #{slot}")
end
```

<!-- livebook:{"output":true} -->

```
1 => 9842
2 => 5649
3 => 1584
4 => 14039
5 => 9974
```

<!-- livebook:{"output":true} -->

```
[:ok, :ok, :ok, :ok, :ok]
```

Even though these keys are sequential numbers, the hashes are (ideally) randomly distributed. 
This ensures each of the nodes have roughly the same number of key-value pairs.
Let's put some data in the cluster to confirm.

```elixir
pairs = [
  {"1", "one"},
  {"2", "two"},
  {"3", "three"},
  {"4", "four"},
  {"5", "five"},
  {"6", "six"},
  {"7", "seven"},
  {"8", "eight"},
  {"9", "nine"},
  {"10", "ten"}
]

for {k, v} <- pairs do
  RedisCluster.Cluster.set(config, k, v)
  slot = RedisCluster.Key.hash_slot(k)
  IO.puts("{#{k}, #{v}} => #{slot}")
end
```

<!-- livebook:{"output":true} -->

```
{1, one} => 9842
{2, two} => 5649
{3, three} => 1584
{4, four} => 14039
{5, five} => 9974
{6, six} => 5781
{7, seven} => 1716
{8, eight} => 14171
{9, nine} => 10106
{10, ten} => 247
```

<!-- livebook:{"output":true} -->

```
[:ok, :ok, :ok, :ok, :ok, :ok, :ok, :ok, :ok, :ok]
```

```elixir
config
|> RedisCluster.Cluster.broadcast([~w[DBSIZE]], role: :master)
|> Enum.sort()
```

<!-- livebook:{"output":true} -->

```
[{"127.0.0.1", 7005, {:ok, [2]}}, {"127.0.0.1", 7010, {:ok, [5]}}, {"127.0.0.1", 7011, {:ok, [3]}}]
```

As long as only the above 10 keys have been written to this cluster, there will be 2 nodes with 2 keys and 2 nodes with 3 keys. That's as even as it gets for such a small set of keys.

## Hash Tags

Next, let's look at a caveat with Redis clusters. Previously, we saw that keys 1, 5, and 9 are on the same node (range 8192-12,287).
So that means we should be able to fetch them in one request with [MGET](https://redis.io/docs/latest/commands/mget/), right?

```elixir
RedisCluster.Cluster.command(config, ~w[MGET 1 5 9], "1", [])
```

<!-- livebook:{"output":true} -->

```
{:error, %Redix.Error{message: "CROSSSLOT Keys in request don't hash to the same slot"}}
```

Wrong. We get the error `CROSSSLOT Keys in request don't hash to the same slot`. 
This means that the keys must hash to the exact value, even if they happen to live on the same node.
With hashes randomly distributed, there's only 1:16,384 change that any two arbitrary keys have the same hash slot. 
So, it seems that we can't use MGET, right?

Wrong. There is a viable option. We can use hash tags.

A hash tag specifies a subset of the key to use for calculating the hash slot. 
Specifically, the first substring surrounded with braces (`{...}`).
It looks like this:

```elixir
keys = [
  "{user1234}:orders",
  "{user1234}:search_history",
  "{user1234}:contact_info",
]

for key <- keys do
  RedisCluster.Key.hash_slot(key, compute_hash_tag: true)
end
```

<!-- livebook:{"output":true} -->

```
[14020, 14020, 14020]
```

All three keys have the same hash slot: 14,020. 
This is the same as if you calculated the hash slot of `user1234`.

```elixir
RedisCluster.Key.hash_slot("user1234")
```

<!-- livebook:{"output":true} -->

```
14020
```

Notice the extra `:compute_hash_tag` option.
The `RedisCluster` library doesn't look for a hash tag unless you request it. 
This avoids any unnecessary overhead if you don't use hash tags.

Also, note that it doesn't matter where in the key the hash tag appears.

```elixir
keys = [
  "orders:{user1234}",
  "search_history:{user1234}",
  "contact_info:{user1234}",
]

for key <- keys do
  RedisCluster.Key.hash_slot(key, compute_hash_tag: true)
end
```

<!-- livebook:{"output":true} -->

```
[14020, 14020, 14020]
```

You can also confirm this behavior by calling the [`CLUSTER KEYSLOT <key>` command](https://redis.io/docs/latest/commands/cluster-keyslot/).

```elixir
for key <- keys do
  RedisCluster.Cluster.command(config, ["CLUSTER", "KEYSLOT", key], "arbitrary", [])
end
```

<!-- livebook:{"output":true} -->

```
[14020, 14020, 14020]
```

You'll notice the extra `:key` option. This helps the `RedisCluster` library know which node to call.
Though in this case it doesn't matter. 
Any node receiving this command can respond appropriately. 
The node isn't looking up the data, it's just computing the slot.

<!-- livebook:{"break_markdown":true} -->

Let's try `MGET` again.

```elixir
set_cmd =  ~w/MSET {user1234}:orders "[]" {user1234}:search_history ["redis"] {user1234}:contact_info [test@example.com]/
RedisCluster.Cluster.command(config, set_cmd, "user1234", compute_hash_tag: true)

get_cmd = ~w[MGET {user1234}:contact_info {user1234}:orders {user1234}:search_history]
RedisCluster.Cluster.command(config, get_cmd, "user1234", compute_hash_tag: true)
```

<!-- livebook:{"output":true} -->

```
["[test@example.com]", "\"[]\"", "[\"redis\"]"]
```

### Binary Keys

You can use binary data as a key. 
However, you need to watch out for the rare chance you have a `{` character (hex `0x7B`)) followed by a `}` character (hex `0x7C`). 
Because of this, it's recommended to prepend `{}` to the start of your key.
When an empty hash tag is found, the full key is always used.

```elixir
binary_key = "{}" <> <<0xDE, 0xAD, 0xBE, 0xEF>>
RedisCluster.Key.hash_slot(binary_key, compute_hash_tag: true)
```

<!-- livebook:{"output":true} -->

```
16021
```

```elixir
cmd = ["CLUSTER", "KEYSLOT", binary_key]
RedisCluster.Cluster.command(config, cmd, binary_key, compute_hash_tag: true)
```

<!-- livebook:{"output":true} -->

```
16021
```

### Hash Data Type

Instead of using hashtags to ensure related data is stored on the same node, you can use a Hash. The Redis folks must really love the word "hash". A Hash is also known as a hash table, map, or dictionary.

```elixir
set_cmd =  ~w/HSET user1234 orders "[]" search_history ["redis"] contact_info [test@example.com]/
RedisCluster.Cluster.command(config, set_cmd, "user1234", [])
```

<!-- livebook:{"output":true} -->

```
3
```

```elixir
RedisCluster.Cluster.command(config, ~w[HKEYS user1234], "user1234", [])
```

<!-- livebook:{"output":true} -->

```
["orders", "search_history", "contact_info"]
```

```elixir
RedisCluster.Cluster.command(config, ~w[HGET user1234 search_history], "user1234", [])
```

<!-- livebook:{"output":true} -->

```
"[\"redis\"]"
```

Redis supports other data types such as 
[lists](https://redis.io/docs/latest/commands/?group=list), 
[sets](https://redis.io/docs/latest/commands/?group=set), 
[sorted sets](https://redis.io/docs/latest/commands/?group=sorted-set), 
[lua scripts](https://redis.io/docs/latest/commands/?group=scripting), 
[bloom filters](https://redis.io/docs/latest/commands/?group=bf), and more.
Check them out in the [docs](https://redis.io/docs/latest/).

## Discovery

So, how does the `RedisCluster` library know which node to send a command to?
This is where the node discovery process comes in. 
The `RedisCluster` library is configured with a URL to connect to a node. 
Ideally this URL should be a "configuration endpoint" as AWS ElastiCache calls it.
The configuration endpoint picks a random node to connect to. 
This ensures one node isn't being hit every time the cluster needs to be discovered.

The `RedisCluster` library sends a [`CLUSTER SHARDS` command](https://redis.io/docs/latest/commands/cluster-shards/)
to the connected node. 
Or for pre-v7 nodes, the [`CLUSTER SLOTS` command](https://redis.io/docs/latest/commands/cluster-slots/).

```elixir
RedisCluster.Cluster.command(config, ~w[CLUSTER SHARDS], "arbitrary", [])
```

<!-- livebook:{"output":true} -->

```
[
  [
    "slots",
    [5461, 10922],
    "nodes",
    [
      ["id", "9133e50abf9938022ef241d30f993fa1d2d20fcc", "port", 7001, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 119736, "health",
       "online"],
      ["id", "ec153174d253129fcb2e30648b94dbb041695a49", "port", 7009, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 119736, "health",
       "online"],
      ["id", "0874c197de5d54b64eff4b337ea3fc52d1f3bd8b", "port", 7007, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 119736, "health",
       "online"],
      ["id", "6add99a58ad4abe9a735e5dfb97c739031344c98", "port", 7010, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "master", "replication-offset", 119736, "health", "online"]
    ]
  ],
  [
    "slots",
    [0, 5460],
    "nodes",
    [
      ["id", "ece6a1885e580b6e298b0fa31c1ef953fd8af5fa", "port", 7000, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 119567, "health",
       "online"],
      ["id", "ed625459e813f8861ed9ddf8637a063a7a86a18e", "port", 7003, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 119567, "health",
       "online"],
      ["id", "6b5b15cc61778653a2946cf90cabf9269f8d0441", "port", 7006, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 119567, "health",
       "online"],
      ["id", "78f7b077c7bf408e1d513009726d9432111018a2", "port", 7011, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "master", "replication-offset", 119567, "health", "online"]
    ]
  ],
  [
    "slots",
    [10923, 16383],
    "nodes",
    [
      ["id", "e25ab6112b5cecc7ff3110e4bebc2eb5283a9e46", "port", 7002, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 120012, "health",
       "online"],
      ["id", "9203ca480260ef729ff41718bbe62c7e6b713f47", "port", 7004, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 120012, "health",
       "online"],
      ["id", "7cdcbc70db636f61bab7493de380f0e76b96c687", "port", 7008, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "replica", "replication-offset", 120012, "health",
       "online"],
      ["id", "1fb93d74adad34737aff38e64934d0753f567982", "port", 7005, "ip", "127.0.0.1",
       "endpoint", "127.0.0.1", "role", "master", "replication-offset", 120012, "health", "online"]
    ]
  ]
]
```

That's quite a bit of data to sift through. 
You can see it in a nicer form with `RedisCluster.HashSlots.all_slots/1`.

```elixir
config
|> RedisCluster.HashSlots.all_slots()
|> Enum.sort()
```

<!-- livebook:{"output":true} -->

```
[
  {RedisCluster.HashSlots, 0, 5460, :master, "127.0.0.1", 7011},
  {RedisCluster.HashSlots, 0, 5460, :replica, "127.0.0.1", 7000},
  {RedisCluster.HashSlots, 0, 5460, :replica, "127.0.0.1", 7003},
  {RedisCluster.HashSlots, 0, 5460, :replica, "127.0.0.1", 7006},
  {RedisCluster.HashSlots, 5461, 10922, :master, "127.0.0.1", 7010},
  {RedisCluster.HashSlots, 5461, 10922, :replica, "127.0.0.1", 7001},
  {RedisCluster.HashSlots, 5461, 10922, :replica, "127.0.0.1", 7007},
  {RedisCluster.HashSlots, 5461, 10922, :replica, "127.0.0.1", 7009},
  {RedisCluster.HashSlots, 10923, 16383, :master, "127.0.0.1", 7005},
  {RedisCluster.HashSlots, 10923, 16383, :replica, "127.0.0.1", 7002},
  {RedisCluster.HashSlots, 10923, 16383, :replica, "127.0.0.1", 7004},
  {RedisCluster.HashSlots, 10923, 16383, :replica, "127.0.0.1", 7008}
]
```

If the `RedisCluster.Cluster` module sends a command and sees a `MOVED` error, ex. `MOVED 3720 127.0.0.1:7000`, then it assumes the cluster topology changed.
It then tries to rediscover the cluster.
For most commands it will retry the command once more.
If it fails again, then there could be a problem with the cluster or the connection to it.

Ideally a node will only have one range of slots. 
However, adding and removing nodes can cause the ranges to become fragmented.
The `RedisCluster` library is smart enough to detect this. 
It creates connections based on the nodes, not the hash ranges.

<!-- livebook:{"break_markdown":true} -->

### Lookup

Knowing which nodes own which slots, the `RedisCluster` library is ready to look up the appropriate node.
This is the process:

1. Hash the key, account for hashtag when appropriate.
2. Look up the nodes that own this hash slot.
3. Filter nodes by role, if needed.
4. Pick a node, if more than one option available.
5. Grab a connection from the node's connection pool.

Note that when the `RedisCluster` library picks a node and connection it does so consistently per process.
It does this with a simple hash of the pid, modulo the node or connection count.

<!-- livebook:{"force_markdown":true} -->

```elixir
index = :erlang.phash2(self(), count)
```

## Using RedisCluster

The prior examples used `RedisCluster.Cluster` directly. 
Though you will likely interact with it using a custom module. 
Start with adding your config. 
This could be in `config.exs` but more likely you will use env vars in `runtime.exs`.

```elixir
Application.put_all_env(myapp: [{MyApp.Redis, [host: "localhost", port: 7000, pool_size: 3]}])
```

<!-- livebook:{"output":true} -->

```
:ok
```

Then you create your own module that uses the `RedisCluster` module.
The `config()` function is available for convenience.

```elixir
defmodule MyApp.Redis do
  use RedisCluster, otp_app: :myapp

  def hset(key, pairs) do
    pairs = Enum.flat_map(pairs, fn {k, v} -> [k, v] end)
    cmd = ["HSET", key | pairs]

    RedisCluster.Cluster.command(config(), cmd, key, [])
  end

  def hkeys(key) do
    RedisCluster.Cluster.command(config(), ["HKEYS", key], key, [])
  end

  def hget(key, field) do
    RedisCluster.Cluster.command(config(), ["HGET", key, field], key, [])
  end

  def hdel(_key, []) do
    0
  end

  def hdel(key, fields) do
    RedisCluster.Cluster.command(config(), ["HDEL", key | fields], key, [])
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, MyApp.Redis, <<70, 79, 82, 49, 0, 0, 35, ...>>, {:hdel, 2}}
```

The `RedisCluster` module will automatically add common functions such as `get/2`, `set/3`, `command/2`, and `pipeline/2`. 
Redis has over 200 commands.
You can add your own convenience functions depending on which Redis commands your application needs.
Notice the above module adds `hset/2`, `hkeys/1`, and `hget/2`.

```elixir
RedisCluster.Cluster.command(config, ~w[COMMAND COUNT], "anything", [])
```

<!-- livebook:{"output":true} -->

```
265
```

Don't forget to add your module to your supervision tree.
For this Livebook we'll start it directly.

```elixir
case MyApp.Redis.start_link([]) do
  {:ok, pid} -> pid
  {:error, {:already_started, pid}} -> pid
end
```

<!-- livebook:{"output":true} -->

```
#PID<0.474.0>
```

From here you can call it directly. 
Notice how much nicer the Hash data type is to work with using the convenience functions we added.

```elixir
MyApp.Redis.hset("myhash", %{a: "1", b: "2", c: "3"})
```

<!-- livebook:{"output":true} -->

```
3
```

```elixir
MyApp.Redis.hkeys("myhash")
```

<!-- livebook:{"output":true} -->

```
["c", "b", "a"]
```

```elixir
MyApp.Redis.hget("myhash", "a")
```

<!-- livebook:{"output":true} -->

```
"1"
```

```elixir
MyApp.Redis.hdel("myhash", ~w[a b c])
```

<!-- livebook:{"output":true} -->

```
3
```

```elixir
MyApp.Redis.hget("myhash", "a")
```

<!-- livebook:{"output":true} -->

```
nil
```

The `RedisCluster.Cluster` module also has `get_many/2`, `set_many/2`, and `delete_many/2` functions.
These are not one-to-one mappings with the 
[`MGET`](https://redis.io/docs/latest/commands/mget/), 
[`MSET`](https://redis.io/docs/latest/commands/mset/), and 
[`DEL`](https://redis.io/docs/latest/commands/del/) commands.

Instead `RedisCluster` tries to be smarter by grouping commands by nodes and sending them in a batch pipeline.
These commands will fail if the cluster reshards while the commands are in flight.
The commands are also sent to each node sequentially for simplicity. 
This may not be as fast as sending the commands in parallel

```elixir
MyApp.Redis.set_many(one: 1, two: 2, three: 3, four: 4, five: 5, six: 6)
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
MyApp.Redis.get_many(~w[six five four three two one])
```

<!-- livebook:{"output":true} -->

```
["6", "5", "4", "3", "2", "1"]
```

```elixir
MyApp.Redis.delete_many(~w[six five four three two one])
```

<!-- livebook:{"output":true} -->

```
6
```

```elixir
MyApp.Redis.get_many(~w[six five four three two one])
```

<!-- livebook:{"output":true} -->

```
[nil, nil, nil, nil, nil, nil]
```

## Read Replicas

Unlike other Elixir Redis libraries, the `RedisCluster` library gives you full access to replica nodes.
This allows your Redis cluster to scale better.

Write commands **must** go to a master node.

```elixir
MyApp.Redis.command(~w[SET fail fail], "fail", role: :replica)
```

<!-- livebook:{"output":true} -->

```
{:error, %Redix.Error{message: "MOVED 3720 127.0.0.1:7011"}}
```

Writing to a replica gives an error like `MOVED 3720 127.0.0.1:7000`.
In this case the replica node is redirecting you to its respective master node.

By default, the `get/2` function sends the `GET` command to a replica. 
Though you can specify it to go to a master node if you don't want any chance of stale data.

```elixir
MyApp.Redis.set("key", "value")
MyApp.Redis.get("key", role: :master)
```

<!-- livebook:{"output":true} -->

```
"value"
```

Or if you don't care which node it goes to, then you can specify `:any`.

```elixir
MyApp.Redis.get("key", role: :any)
```

<!-- livebook:{"output":true} -->

```
"value"
```
